{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk library along with the TfidfVectorizer was used to preprocess the data. The titles were tokenized, punctuations etc. with non-alphabetic characters as well as stop words were removed. The data was then lemmatized and the dictionary consisting of only more frequent words was created. This dictionary along with preprocessed data was then used in the TfidfVectorizer to create input X to be used in various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('redditData1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for t in data['title']:\n",
    "    word_list = word_list + word_tokenize(t.lower())\n",
    "    \n",
    "from collections import Counter\n",
    "common_words = list(Counter(word_list).most_common(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sorted(list(set(word_list)))\n",
    "word_list = [item for item in word_list if item.isalpha()]\n",
    "\n",
    "word_list = [x[0] for x in common_words]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list_sw = [w for w in word_list if not w in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_list_lem = [lemmatizer.lemmatize(w) for w in word_list_sw]\n",
    "word_list_lem = sorted(list(set(word_list_lem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'corona\",\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '/',\n",
       " '1',\n",
       " '1,000',\n",
       " '1,500',\n",
       " '10',\n",
       " '10,000',\n",
       " '100',\n",
       " '1000',\n",
       " '10th',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '14th',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '1st',\n",
       " '2',\n",
       " '2.0',\n",
       " '20',\n",
       " '20,000',\n",
       " '200',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '2020',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '2yodoindia',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '31',\n",
       " '39',\n",
       " '3d',\n",
       " '3rd',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '48',\n",
       " '480p',\n",
       " '4th',\n",
       " '5',\n",
       " '5.7',\n",
       " '50',\n",
       " '50,000',\n",
       " '500',\n",
       " '54',\n",
       " '5th',\n",
       " '6',\n",
       " '60',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '9.99',\n",
       " '90',\n",
       " '93',\n",
       " '9pm',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " ']',\n",
       " '``',\n",
       " 'aap',\n",
       " 'aarogya',\n",
       " 'abhijit',\n",
       " 'able',\n",
       " 'abp',\n",
       " 'abroad',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accused',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'address',\n",
       " 'adityanath',\n",
       " 'administration',\n",
       " 'admit',\n",
       " 'advice',\n",
       " 'advisory',\n",
       " 'affair',\n",
       " 'affected',\n",
       " 'afghanistan',\n",
       " 'africa',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'ago',\n",
       " 'agra',\n",
       " 'agree',\n",
       " 'ahead',\n",
       " 'ahmedabad',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aiims',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'akshay',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'ali',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'ambani',\n",
       " 'ambedkar',\n",
       " 'ambulance',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amit',\n",
       " 'amitabh',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'analysis',\n",
       " 'anand',\n",
       " 'ancient',\n",
       " 'andaman',\n",
       " 'andhra',\n",
       " 'android',\n",
       " 'angry',\n",
       " 'ani',\n",
       " 'animal',\n",
       " 'anniversary',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anti',\n",
       " 'anti-malarial',\n",
       " 'anti-muslim',\n",
       " 'antibody',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'apartment',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'approved',\n",
       " 'approves',\n",
       " 'apps',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'area',\n",
       " 'army',\n",
       " 'arnab',\n",
       " 'around',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'art',\n",
       " 'article',\n",
       " 'artist',\n",
       " 'arunachal',\n",
       " 'arundhati',\n",
       " 'arvind',\n",
       " 'asha',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'askindia',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'assam',\n",
       " 'assistance',\n",
       " 'association',\n",
       " 'atleast',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacking',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attended',\n",
       " 'attendee',\n",
       " 'audio',\n",
       " 'aur',\n",
       " 'authority',\n",
       " 'auto',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'ayurveda',\n",
       " 'ayurvedic',\n",
       " 'ayush',\n",
       " 'b',\n",
       " 'baat',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'background',\n",
       " 'bad',\n",
       " 'bagh',\n",
       " 'bail',\n",
       " 'balcony',\n",
       " 'ban',\n",
       " 'bandra',\n",
       " 'banerjee',\n",
       " 'bangalore',\n",
       " 'bank',\n",
       " 'banned',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basis',\n",
       " 'battle',\n",
       " 'bay',\n",
       " 'bcg',\n",
       " 'bcom',\n",
       " 'beach',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beating',\n",
       " 'beautiful',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'bed',\n",
       " 'begin',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'bell',\n",
       " 'benefit',\n",
       " 'bengal',\n",
       " 'bengaluru',\n",
       " 'best',\n",
       " 'better',\n",
       " 'beyond',\n",
       " 'bhai',\n",
       " 'bhakts',\n",
       " 'bharat',\n",
       " 'bhi',\n",
       " 'bhilwara',\n",
       " 'bhopal',\n",
       " 'biased',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'bihar',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'bjp',\n",
       " 'black',\n",
       " 'blackout',\n",
       " 'blame',\n",
       " 'blaming',\n",
       " 'blood',\n",
       " 'bmc',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'body',\n",
       " 'bollywood',\n",
       " 'bombay',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'boost',\n",
       " 'border',\n",
       " 'bored',\n",
       " 'born',\n",
       " 'boy',\n",
       " 'boycott',\n",
       " 'brand',\n",
       " 'breaking',\n",
       " 'bring',\n",
       " 'brings',\n",
       " 'british',\n",
       " 'broadband',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brutality',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'burial',\n",
       " 'burning',\n",
       " 'bus',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'ca',\n",
       " 'cabinet',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'campbell',\n",
       " 'canada',\n",
       " 'cancelled',\n",
       " 'cancer',\n",
       " 'candle',\n",
       " 'capacity',\n",
       " 'capital',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'career',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'caste',\n",
       " 'cat',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'cbse',\n",
       " 'celebrate',\n",
       " 'celebrating',\n",
       " 'cell',\n",
       " 'cent',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'challenge',\n",
       " 'chandigarh',\n",
       " 'change',\n",
       " 'channel',\n",
       " 'chaos',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'charity',\n",
       " 'check',\n",
       " 'chennai',\n",
       " 'chhattisgarh',\n",
       " 'chicken',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'choose',\n",
       " 'citizen',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'clapping',\n",
       " 'clash',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'cleaner',\n",
       " 'clear',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'cloud',\n",
       " 'cm',\n",
       " 'collected',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'combat',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'commits',\n",
       " 'committed',\n",
       " 'committee',\n",
       " 'common',\n",
       " 'communal',\n",
       " 'community',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'comparison',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'compulsory',\n",
       " 'concern',\n",
       " 'condition',\n",
       " 'conduct',\n",
       " 'conducted',\n",
       " 'conducting',\n",
       " 'conference',\n",
       " 'conferencing',\n",
       " 'confirm',\n",
       " 'confirmed',\n",
       " 'congregation',\n",
       " 'congress',\n",
       " 'connection',\n",
       " 'considered',\n",
       " 'consignment',\n",
       " 'conspiracy',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'containment',\n",
       " 'content',\n",
       " 'continue',\n",
       " 'continues',\n",
       " 'contribute',\n",
       " 'control',\n",
       " 'convert',\n",
       " 'cooked',\n",
       " 'cool',\n",
       " 'cop',\n",
       " 'copyright',\n",
       " 'corona',\n",
       " 'coronavirus',\n",
       " 'corporate',\n",
       " 'corporation',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'council',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cousin',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'covid',\n",
       " 'covid-19',\n",
       " 'covid19',\n",
       " 'cow',\n",
       " 'cr',\n",
       " 'cracker',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'credit',\n",
       " 'cricket',\n",
       " 'crime',\n",
       " 'crisis',\n",
       " 'criticism',\n",
       " 'crore',\n",
       " 'cross',\n",
       " 'crowd',\n",
       " 'crude',\n",
       " 'culture',\n",
       " 'cure',\n",
       " 'cured',\n",
       " 'curfew',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'curve',\n",
       " 'cut',\n",
       " 'dad',\n",
       " 'daily',\n",
       " 'dakshineswar',\n",
       " 'dal',\n",
       " 'dalit',\n",
       " 'dance',\n",
       " 'dangerous',\n",
       " 'dark',\n",
       " 'darkness',\n",
       " 'data',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'dd',\n",
       " 'dead',\n",
       " 'deadly',\n",
       " 'deal',\n",
       " 'dealing',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'deccan',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'declared',\n",
       " 'deep',\n",
       " 'defend',\n",
       " 'defying',\n",
       " 'delhi',\n",
       " 'deliver',\n",
       " 'delivering',\n",
       " 'delivery',\n",
       " 'demand',\n",
       " 'demanding',\n",
       " 'democracy',\n",
       " 'denied',\n",
       " 'denies',\n",
       " 'department',\n",
       " 'depression',\n",
       " 'deserve',\n",
       " 'desi',\n",
       " 'design',\n",
       " 'despite',\n",
       " 'detail',\n",
       " 'develop',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'develops',\n",
       " 'devotee',\n",
       " 'dharavi',\n",
       " 'die',\n",
       " 'died',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'directly',\n",
       " 'director',\n",
       " 'disaster',\n",
       " 'discharged',\n",
       " 'discord',\n",
       " 'discrimination',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disinfectant',\n",
       " 'distance',\n",
       " 'distancing',\n",
       " 'distribution',\n",
       " 'district',\n",
       " 'diverted',\n",
       " 'diwali',\n",
       " 'diy',\n",
       " 'diya',\n",
       " 'diyas',\n",
       " 'dm',\n",
       " 'doc',\n",
       " 'doctor',\n",
       " 'documentary',\n",
       " 'dog',\n",
       " 'domestic',\n",
       " 'donald',\n",
       " 'donate',\n",
       " 'donated',\n",
       " 'donates',\n",
       " 'donating',\n",
       " 'donation',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'doordarshan',\n",
       " 'doubling',\n",
       " 'download',\n",
       " 'dr',\n",
       " 'dr.',\n",
       " 'draw',\n",
       " 'dream',\n",
       " 'drinking',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drone',\n",
       " 'drop',\n",
       " 'dropped',\n",
       " 'drug',\n",
       " 'dubai',\n",
       " 'due',\n",
       " 'duty',\n",
       " 'dy',\n",
       " 'e-commerce',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earn',\n",
       " 'earth',\n",
       " 'earthquake',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'economic',\n",
       " 'economy',\n",
       " 'edition',\n",
       " 'editor',\n",
       " 'education',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effort',\n",
       " 'eight',\n",
       " 'election',\n",
       " 'electricity',\n",
       " 'else',\n",
       " 'emergency',\n",
       " 'emi',\n",
       " 'employee',\n",
       " 'end',\n",
       " 'energy',\n",
       " 'enforce',\n",
       " 'engineering',\n",
       " 'english',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'ensure',\n",
       " 'entertainment',\n",
       " 'entire',\n",
       " 'entrance',\n",
       " 'entry',\n",
       " 'environment',\n",
       " 'epidemic',\n",
       " 'episode',\n",
       " 'equipment',\n",
       " 'equivalent',\n",
       " 'especially',\n",
       " 'essential',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'exam',\n",
       " 'exclusive',\n",
       " 'exit',\n",
       " 'exodus',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'experience',\n",
       " 'expert',\n",
       " 'explain',\n",
       " 'explained',\n",
       " 'explains',\n",
       " 'export',\n",
       " 'exposure',\n",
       " 'express',\n",
       " 'extend',\n",
       " 'extended',\n",
       " 'extending',\n",
       " 'extends',\n",
       " 'extension',\n",
       " 'extremely',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'faced',\n",
       " 'facility',\n",
       " 'facing',\n",
       " 'fact',\n",
       " 'fail',\n",
       " 'failed',\n",
       " 'failure',\n",
       " 'fair',\n",
       " 'fake',\n",
       " 'fall',\n",
       " 'false',\n",
       " 'falsely',\n",
       " 'family',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'farmer',\n",
       " 'fast',\n",
       " 'father',\n",
       " 'fb',\n",
       " 'fdi',\n",
       " 'fear',\n",
       " 'february',\n",
       " 'fee',\n",
       " 'feed',\n",
       " 'feeding',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'fellow',\n",
       " 'felt',\n",
       " 'female',\n",
       " 'festival',\n",
       " 'fewer',\n",
       " 'field',\n",
       " 'fight',\n",
       " 'fighting',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'filed',\n",
       " 'fill',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'finance',\n",
       " 'financial',\n",
       " 'find',\n",
       " 'finding',\n",
       " 'fine',\n",
       " 'fir',\n",
       " 'fire',\n",
       " 'firecracker',\n",
       " 'firm',\n",
       " 'first',\n",
       " 'firstpost',\n",
       " 'fit',\n",
       " 'five',\n",
       " 'flag',\n",
       " 'flatten',\n",
       " 'flight',\n",
       " 'flu',\n",
       " 'folk',\n",
       " 'follow',\n",
       " 'follower',\n",
       " 'following',\n",
       " 'food',\n",
       " 'force',\n",
       " 'forced',\n",
       " 'forcing',\n",
       " 'foreign',\n",
       " 'foreigner',\n",
       " 'forget',\n",
       " 'form',\n",
       " 'former',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'foundation',\n",
       " 'four',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'front',\n",
       " 'fruit',\n",
       " 'ft.',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'fund',\n",
       " 'funeral',\n",
       " 'funny',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gandhi',\n",
       " 'ganga',\n",
       " 'gate',\n",
       " 'gather',\n",
       " 'gathered',\n",
       " 'gathering',\n",
       " 'gave',\n",
       " 'gdp',\n",
       " 'gear',\n",
       " 'general',\n",
       " 'genius',\n",
       " 'genuine',\n",
       " 'germany',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'ghaziabad',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'global',\n",
       " 'go',\n",
       " 'goa',\n",
       " 'god',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'gon',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'google',\n",
       " 'goon',\n",
       " 'goswami',\n",
       " 'got',\n",
       " 'government',\n",
       " 'govt',\n",
       " 'graph',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'grid',\n",
       " 'grocery',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'growing',\n",
       " 'growth',\n",
       " 'guess',\n",
       " 'guide',\n",
       " 'guideline',\n",
       " 'guitar',\n",
       " 'gujarat',\n",
       " 'guru',\n",
       " 'gurudwara',\n",
       " 'guwahati',\n",
       " 'guy',\n",
       " 'hai',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'handling',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'haryana',\n",
       " 'hate',\n",
       " 'hatred',\n",
       " 'hc',\n",
       " 'hcq',\n",
       " 'hd',\n",
       " 'hdfc',\n",
       " 'head',\n",
       " 'health',\n",
       " 'healthcare',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heart',\n",
       " 'held',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'helping',\n",
       " 'helpline',\n",
       " 'herald',\n",
       " 'herd',\n",
       " 'hero',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'hiding',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'highly',\n",
       " 'hilarious',\n",
       " 'himachal',\n",
       " 'himalaya',\n",
       " 'hindi',\n",
       " 'hindu',\n",
       " 'hindutva',\n",
       " 'hiring',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'ho',\n",
       " 'hold',\n",
       " 'holder',\n",
       " 'home',\n",
       " 'homemade',\n",
       " 'honest',\n",
       " 'hope',\n",
       " 'hospital',\n",
       " 'hotel',\n",
       " 'hotspot',\n",
       " 'hotstar',\n",
       " 'hour',\n",
       " 'house',\n",
       " 'http',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'humanity',\n",
       " 'hundred',\n",
       " 'hunger',\n",
       " 'hungry',\n",
       " 'husband',\n",
       " 'hyderabad',\n",
       " 'hydroxychloroquine',\n",
       " 'hypocrisy',\n",
       " 'ia',\n",
       " 'icmr',\n",
       " 'icon',\n",
       " 'idea',\n",
       " 'idiot',\n",
       " 'ignore',\n",
       " 'iit',\n",
       " 'illegal',\n",
       " 'illness',\n",
       " 'im',\n",
       " 'image',\n",
       " 'imagine',\n",
       " 'imf',\n",
       " 'immediately',\n",
       " 'immunity',\n",
       " 'impact',\n",
       " 'important',\n",
       " 'improve',\n",
       " 'improves',\n",
       " 'incident',\n",
       " 'including',\n",
       " 'income',\n",
       " 'increase',\n",
       " 'increased',\n",
       " 'increasing',\n",
       " 'independent',\n",
       " 'india',\n",
       " 'indian',\n",
       " 'indoors',\n",
       " 'indore',\n",
       " 'industry',\n",
       " 'infected',\n",
       " 'infection',\n",
       " 'influence',\n",
       " 'info',\n",
       " 'information',\n",
       " 'initiative',\n",
       " 'injured',\n",
       " 'innovation',\n",
       " 'inside',\n",
       " 'inspired',\n",
       " 'insta',\n",
       " 'instagram',\n",
       " 'install',\n",
       " 'instead',\n",
       " 'institute',\n",
       " 'insurance',\n",
       " 'intelligence',\n",
       " 'interest',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'inventiva',\n",
       " 'investment',\n",
       " 'ipl',\n",
       " 'islam',\n",
       " 'islamic',\n",
       " 'islamophobia',\n",
       " 'islamophobic',\n",
       " 'island',\n",
       " 'isolation',\n",
       " 'issue',\n",
       " 'issued',\n",
       " 'italy',\n",
       " 'item',\n",
       " 'j',\n",
       " 'jai',\n",
       " 'jail',\n",
       " 'jain',\n",
       " 'jaipur',\n",
       " 'jalandhar',\n",
       " 'jamaat',\n",
       " 'jamat',\n",
       " 'jamia',\n",
       " 'jammu',\n",
       " 'jan',\n",
       " 'january',\n",
       " 'jharkhand',\n",
       " 'ji',\n",
       " 'jio',\n",
       " 'jnu',\n",
       " 'job',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict.pkl', 'wb') as f:\n",
    "    pickle.dump(word_list_lem, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    result = []\n",
    "    for s in content:\n",
    "        data = word_tokenize(s.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        data = [w for w in data if not w in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        data_lem = [lemmatizer.lemmatize(w) for w in data]\n",
    "        data_lem = sorted(list(set(data_lem)))\n",
    "        \n",
    "        data = ' '.join(word for word in data_lem)\n",
    "        result.append(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TfidfVectorizer(input = 'content', vocabulary = word_list_lem)\n",
    "X = clf.fit_transform(preprocess(data['title']))\n",
    "#X = clf.fit_transform(data['title'])\n",
    "clf.get_feature_names()\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12063, 2128)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary was used to map the flair labels to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs = data['link_flair_text'].unique()\n",
    "f_dict = dict(zip(flairs, range(len(flairs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Non-Political': 0,\n",
       " 'Coronavirus': 1,\n",
       " 'AskIndia': 2,\n",
       " 'Photography': 3,\n",
       " 'Science/Technology': 4,\n",
       " 'Politics': 5,\n",
       " 'Policy/Economy': 6,\n",
       " 'Business/Finance': 7,\n",
       " 'Sports': 8,\n",
       " 'Food': 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.applymap(lambda s: f_dict.get(s) if s in f_dict else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['link_flair_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12063,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was then split into training and testing sets. SVC and Random Forest was used. However, since the dataset was not balanced, most of the times, the majority class was predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', C=1.0, gamma = 'scale')\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "p = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42470295661785024\n",
      "[[   0  678    0    0    0    0    0    0    0    0]\n",
      " [   0 1537    0    0    0    0    0    0    0    0]\n",
      " [   0  508    0    0    0    0    0    0    0    0]\n",
      " [   0  103    0    0    0    0    0    0    0    0]\n",
      " [   0  133    0    0    0    0    0    0    0    0]\n",
      " [   0  382    0    0    0    0    0    0    0    0]\n",
      " [   0  111    0    0    0    0    0    0    0    0]\n",
      " [   0   95    0    0    0    0    0    0    0    0]\n",
      " [   0   35    0    0    0    0    0    0    0    0]\n",
      " [   0   37    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, p)\n",
    "cm = confusion_matrix(y_test, p)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model=RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_train,y_train)\n",
    "rf_pred=rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5473887814313346\n",
      "[[ 248  275   78   10   10   48    3    2    1    3]\n",
      " [ 100 1306   68    7    6   42    4    2    0    2]\n",
      " [ 109  179  180    5    7   18    1    6    0    3]\n",
      " [  27   20    5   47    0    2    0    0    1    1]\n",
      " [  30   50   17    0   26    3    0    6    1    0]\n",
      " [  55  172   23    1    0  129    2    0    0    0]\n",
      " [  18   55   15    1    1    4   15    2    0    0]\n",
      " [  15   40   17    0    3    0    2   17    0    1]\n",
      " [   8   17    2    0    0    1    0    0    6    1]\n",
      " [   8   15    5    1    0    1    0    0    0    7]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, rf_pred)\n",
    "cm = confusion_matrix(y_test, rf_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tackle the imbalance of the data, the class_weight keyword was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(kernel='rbf', C=1.0, gamma = 'scale', class_weight = 'balanced')\n",
    "    \n",
    "svc_model.fit(X_train, y_train)\n",
    "p = svc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2555954683614258\n",
      "[[ 666    4    1    2    0    0    1    3    0    1]\n",
      " [1321  190    1    0    0    9   12    0    1    3]\n",
      " [ 476   16    1    5    0    1    1    2    2    4]\n",
      " [  83    0    0   19    0    0    0    0    1    0]\n",
      " [ 129    1    0    0    0    0    0    3    0    0]\n",
      " [ 362    6    0    0    0   10    4    0    0    0]\n",
      " [  97    2    0    1    0    0    9    2    0    0]\n",
      " [  77    2    0    0    0    0    2   12    1    1]\n",
      " [  26    1    0    0    0    0    0    0    8    0]\n",
      " [  27    0    0    0    0    0    0    0    0   10]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, p)\n",
    "cm = confusion_matrix(y_test, p)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=RandomForestClassifier(n_estimators=100, class_weight = 'balanced')\n",
    "rf_model.fit(X_train,y_train)\n",
    "rf_pred=rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5288753799392097\n",
      "[[ 233  229   83   18   10   46    5    5    3   46]\n",
      " [ 111 1225   70    7   10   52    9    6    0   47]\n",
      " [ 105  155  174    9    8   26    1   10    2   18]\n",
      " [  31   13    6   46    0    1    0    0    2    4]\n",
      " [  25   32   18    2   37    5    0    5    1    8]\n",
      " [  57  145   18    2    2  141    2    1    0   14]\n",
      " [  13   50   16    1    1    5   17    4    0    4]\n",
      " [  13   35   18    0    1    1    1   19    0    7]\n",
      " [   6   15    2    0    0    1    0    1    8    2]\n",
      " [   5   10    5    1    0    1    0    1    0   14]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, rf_pred)\n",
    "cm = confusion_matrix(y_test, rf_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I tried oversampling from the minority class to create a new training set. This was used to train random forest and XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE('minority')\n",
    "X_s, y_s = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11962, 2128)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_s,y_s)\n",
    "rf_pred=rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5377176015473888\n",
      "[[ 241  277   67    8    7   41    4    3   26    4]\n",
      " [ 100 1300   61    6    7   45    4    3    9    2]\n",
      " [ 101  179  166    7    7   20    2    7   17    2]\n",
      " [  21   22    5   44    0    2    0    0    8    1]\n",
      " [  29   45   14    0   23    5    0    4   13    0]\n",
      " [  63  175   15    1    1  121    2    0    4    0]\n",
      " [  13   59   16    0    0    4   14    3    2    0]\n",
      " [  15   38   14    0    0    0    3   16    7    2]\n",
      " [   7   13    2    0    0    0    0    0   13    0]\n",
      " [   8   12    6    1    0    1    0    0    1    8]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, rf_pred)\n",
    "cm = confusion_matrix(y_test, rf_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_rf.pkl.z', 'wb') as file:\n",
    "    pickle.dump(rf_model, file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method=None,\n",
       "       validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_s, y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred=model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5363360044211108\n",
      "[[ 332  208   62   10    9   38    3    6    9    1]\n",
      " [ 194 1229   43    3    5   45    8    2    7    1]\n",
      " [ 153  153  150    5    6   19    0   10   11    1]\n",
      " [  40   21    2   32    1    0    0    0    7    0]\n",
      " [  46   41   16    0   14    4    0    5    7    0]\n",
      " [  95  141   12    1    1  126    4    0    1    1]\n",
      " [  27   46    9    1    2    3   18    5    0    0]\n",
      " [  25   30   11    0    2    1    1   20    4    1]\n",
      " [  11    8    0    0    0    3    0    1   12    0]\n",
      " [  14   12    3    0    0    0    0    0    0    8]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, xgb_pred)\n",
    "cm = confusion_matrix(y_test, xgb_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "# with open('model_xgb.pkl', 'wb') as file:\n",
    "#     pickle.dump(model_xgb, file, protocol=2)\n",
    "# joblib.dump(model_xgb, 'model.pkl.z', protocol = 2) \n",
    "\n",
    "# pickle.dump(model_xgb, open(\"pima.pickle.dat\", \"wb\"), protocol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then tried a combination of oversampling and undersampling. This, however, did not give improved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_o, y_o = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ou, y_ou = undersample.fit_resample(X_o, y_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12098, 2128)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ou.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method=None,\n",
       "       validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.fit(X_ou, y_ou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4249792760431058\n",
      "[[590   0  35   5   3  25   4   3  11   2]\n",
      " [874  87 211   6  19 250  44  11  29   6]\n",
      " [168   0 314   2   3  13   0   1   7   0]\n",
      " [ 29   0   2  70   0   0   0   0   2   0]\n",
      " [ 51   0  12   0  58   6   0   2   4   0]\n",
      " [122   0   9   0   0 248   2   0   1   0]\n",
      " [ 34   0   5   0   0   4  66   2   0   0]\n",
      " [ 32   0   6   0   0   1   1  51   2   2]\n",
      " [  2   0   0   0   0   0   0   0  33   0]\n",
      " [ 13   0   2   0   0   0   0   0   1  21]]\n"
     ]
    }
   ],
   "source": [
    "xgb_pred=model_xgb.predict(X_test)\n",
    "acc = accuracy_score(y_test, xgb_pred)\n",
    "cm = confusion_matrix(y_test, xgb_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I used Keras to create a fairly simple DNN for the classification of reddit submissions. This, too, did not give as good of a result as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer_obj = Tokenizer()\n",
    "\n",
    "X1 = np.array(df['title'])\n",
    "y1 = np.array(df['link_flair_text'])\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total = np.array(list(X1_train) + list(X1_test))\n",
    "tokenizer_obj.fit_on_texts(total)\n",
    "\n",
    "#pad the sequences\n",
    "max_len = max([len(s.split()) for s in total])\n",
    "\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "X1_train_tokens = tokenizer_obj.texts_to_sequences(X1_train)\n",
    "X1_test_tokens = tokenizer_obj.texts_to_sequences(X1_test)\n",
    "\n",
    "X1_train_pad = pad_sequences(X1_train_tokens, maxlen = max_len, padding = 'post')\n",
    "X1_test_pad = pad_sequences(X1_test_tokens, maxlen = max_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE('minority')\n",
    "X1_s, y1_s = smote.fit_sample(X1_train_pad, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 62, 100)           1763200   \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,776,298\n",
      "Trainable params: 1,776,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMB_DIM = 100\n",
    "num_labels = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMB_DIM, input_length = max_len))\n",
    "model.add(GRU(units = 32, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(num_labels, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "print(\"Model summary:\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.copy(y1_s)\n",
    "y_oh = np.zeros((y.shape[0], 10))\n",
    "for i in range(y.shape[0]):\n",
    "    y_oh[i][y[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10167 samples, validate on 1795 samples\n",
      "Epoch 1/15\n",
      " - 15s - loss: 1.9632 - acc: 0.3328 - val_loss: 1.6807 - val_acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      " - 10s - loss: 1.8420 - acc: 0.3526 - val_loss: 1.7517 - val_acc: 0.0000e+00\n",
      "Epoch 3/15\n",
      " - 11s - loss: 1.8417 - acc: 0.3526 - val_loss: 1.7280 - val_acc: 0.0000e+00\n",
      "Epoch 4/15\n",
      " - 11s - loss: 1.8398 - acc: 0.3526 - val_loss: 1.7578 - val_acc: 0.0000e+00\n",
      "Epoch 5/15\n",
      " - 11s - loss: 1.8397 - acc: 0.3526 - val_loss: 1.7114 - val_acc: 0.0000e+00\n",
      "Epoch 6/15\n",
      " - 11s - loss: 1.8401 - acc: 0.3526 - val_loss: 1.7981 - val_acc: 0.0000e+00\n",
      "Epoch 7/15\n",
      " - 12s - loss: 1.8242 - acc: 0.3526 - val_loss: 1.7708 - val_acc: 0.0000e+00\n",
      "Epoch 8/15\n",
      " - 11s - loss: 1.6935 - acc: 0.4129 - val_loss: 1.6803 - val_acc: 0.4825\n",
      "Epoch 9/15\n",
      " - 11s - loss: 1.5387 - acc: 0.4765 - val_loss: 1.8583 - val_acc: 0.3705\n",
      "Epoch 10/15\n",
      " - 11s - loss: 1.4181 - acc: 0.5204 - val_loss: 2.0282 - val_acc: 0.2953\n",
      "Epoch 11/15\n",
      " - 11s - loss: 1.3011 - acc: 0.5775 - val_loss: 1.9990 - val_acc: 0.3694\n",
      "Epoch 12/15\n",
      " - 11s - loss: 1.1747 - acc: 0.6317 - val_loss: 2.2610 - val_acc: 0.3577\n",
      "Epoch 13/15\n",
      " - 11s - loss: 1.0538 - acc: 0.6710 - val_loss: 2.3923 - val_acc: 0.3682\n",
      "Epoch 14/15\n",
      " - 11s - loss: 0.9503 - acc: 0.6978 - val_loss: 2.8169 - val_acc: 0.3426\n",
      "Epoch 15/15\n",
      " - 11s - loss: 0.8819 - acc: 0.7155 - val_loss: 2.5232 - val_acc: 0.3822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e4958f28d0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "model.fit(X1_s, y_oh, batch_size = batch_size, epochs = num_epochs, verbose = 2, validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.copy(y1_test)\n",
    "y_oh_test = np.zeros((y.shape[0], 10))\n",
    "for i in range(y.shape[0]):\n",
    "    y_oh_test[i][y[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3702680299330825"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = model.evaluate(X1_test_pad, y_oh_test, batch_size = batch_size, verbose = 2)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost and Random Forest models with oversampled data had the best results. The Random Forest model was saved to use in the final application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
